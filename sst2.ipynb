{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook shows how to fine-tune a model on sst-2 dataset and how to distill the model with TextBrewer.\n",
    "\n",
    "Detailed Docs can be find here:\n",
    "https://github.com/airaria/TextBrewer"
   ],
   "metadata": {
    "id": "UMExDS48VN58"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu' \r\n",
    "device"
   ],
   "outputs": [],
   "metadata": {
    "id": "oVTjuvH0rPsT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# https://github.com/airaria/TextBrewer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# !pip install transformers #4.8.2\r\n",
    "# !pip install datasets\r\n",
    "# !pip install textbrewer"
   ],
   "outputs": [],
   "metadata": {
    "id": "yIgAk4WVrKtC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\r\n",
    "import torch\r\n",
    "from transformers import BertForSequenceClassification, BertTokenizer,BertConfig\r\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\r\n",
    "from transformers import Trainer, TrainingArguments\r\n",
    "from datasets import load_dataset,load_metric"
   ],
   "outputs": [],
   "metadata": {
    "id": "qqu-aNtc3QgP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare dataset to train"
   ],
   "metadata": {
    "id": "h5ww8ad58D8v"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_dataset = load_dataset('clinc_oos', 'plus', split='train')\r\n",
    "val_dataset = load_dataset('clinc_oos', 'plus', split='validation')\r\n",
    "test_dataset = load_dataset('clinc_oos', 'plus', split='test')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset clinc_oos (C:\\Users\\Subha\\.cache\\huggingface\\datasets\\clinc_oos\\plus\\1.0.0\\abcc41d382f8137f039adc747af44714941e8196e845dfbdd8ae7a7e020e6ba1)\n",
      "Reusing dataset clinc_oos (C:\\Users\\Subha\\.cache\\huggingface\\datasets\\clinc_oos\\plus\\1.0.0\\abcc41d382f8137f039adc747af44714941e8196e845dfbdd8ae7a7e020e6ba1)\n",
      "Reusing dataset clinc_oos (C:\\Users\\Subha\\.cache\\huggingface\\datasets\\clinc_oos\\plus\\1.0.0\\abcc41d382f8137f039adc747af44714941e8196e845dfbdd8ae7a7e020e6ba1)\n"
     ]
    }
   ],
   "metadata": {
    "id": "9-8wYOHG4WVq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'intent'],\n",
       "    num_rows: 15250\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_dataset = train_dataset.map(lambda examples: {'labels': examples['intent']}, batched=True)\r\n",
    "val_dataset = val_dataset.map(lambda examples: {'labels': examples['intent']}, batched=True)\r\n",
    "test_dataset = test_dataset.map(lambda examples: {'labels': examples['intent']}, batched=True)\r\n",
    "val_dataset = val_dataset.remove_columns(['intent'])\r\n",
    "test_dataset = test_dataset.remove_columns(['intent'])\r\n",
    "train_dataset = train_dataset.remove_columns(['intent'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 432.44ba/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 571.39ba/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 547.61ba/s]\n"
     ]
    }
   ],
   "metadata": {
    "id": "iQSki-hv5Imc"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\r\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {
    "id": "whL22dsx5QU5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "MAX_LENGTH = 128\r\n",
    "train_dataset = train_dataset.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\r\n",
    "val_dataset = val_dataset.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\r\n",
    "test_dataset = test_dataset.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 16/16 [00:03<00:00,  4.63ba/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.28ba/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.61ba/s]\n"
     ]
    }
   ],
   "metadata": {
    "id": "eH4rBumG5i6S"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\r\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\r\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ],
   "outputs": [],
   "metadata": {
    "id": "Nv-gsKvG5ylO"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def compute_metrics(pred):\r\n",
    "    labels = pred.label_ids\r\n",
    "    preds = pred.predictions.argmax(-1)\r\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\r\n",
    "    acc = accuracy_score(labels, preds)\r\n",
    "    return {\r\n",
    "        'accuracy': acc,\r\n",
    "        'f1': f1,\r\n",
    "        'precision': precision,\r\n",
    "        'recall': recall\r\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {
    "id": "6jwP2aHv6EU6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#start training \r\n",
    "training_args = TrainingArguments(\r\n",
    "    output_dir='./results',          #output directory\r\n",
    "    learning_rate=1e-4,\r\n",
    "    num_train_epochs=5,              \r\n",
    "    per_device_train_batch_size=16,                #batch size per device during training\r\n",
    "    per_device_eval_batch_size=16,                #batch size for evaluation\r\n",
    "    logging_dir='./logs',            \r\n",
    "    logging_steps=100,\r\n",
    "    do_train=True,\r\n",
    "    do_eval=True,\r\n",
    "    no_cuda=False,\r\n",
    "    load_best_model_at_end=True,\r\n",
    "    # eval_steps=100,\r\n",
    "    evaluation_strategy=\"epoch\",\r\n",
    "    save_strategy = \"epoch\"\r\n",
    ")\r\n",
    "\r\n",
    "trainer = Trainer(\r\n",
    "    model=model,                         \r\n",
    "    args=training_args,                  \r\n",
    "    train_dataset=train_dataset,         \r\n",
    "    eval_dataset=val_dataset,            \r\n",
    "    compute_metrics=compute_metrics\r\n",
    ")\r\n",
    "\r\n",
    "train_out = trainer.train()\r\n",
    "\r\n",
    "#after training, you could find traing logs and checpoints in your own dirve. also you can reset the file address in training args"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4770\n",
      "  0%|          | 0/4770 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-29625c577361>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mtrain_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#after training, you could find traing logs and checpoints in your own dirve. also you can reset the file address in training args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1284\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1286\u001b[1;33m                     \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1287\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1795\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1796\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1797\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1799\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "metadata": {
    "id": "KonAbPBj6NCK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.save(model.state_dict(), '/content/drive/MyDrive/sst2_teacher_model.pt')\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "1H8Dod2y6R8c"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Start distillation"
   ],
   "metadata": {
    "id": "Gov66CaFNAgg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32) #prepare dataloader"
   ],
   "outputs": [],
   "metadata": {
    "id": "IA-gwQKNB8fs"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import textbrewer\r\n",
    "from textbrewer import GeneralDistiller\r\n",
    "from textbrewer import TrainingConfig, DistillationConfig\r\n",
    "from transformers import BertForSequenceClassification, BertConfig, AdamW,BertTokenizer\r\n",
    "from transformers import get_linear_schedule_with_warmup"
   ],
   "outputs": [],
   "metadata": {
    "id": "YD8qPZmUiTKH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize the student model by BertConfig and prepare the teacher model.\n",
    "\n",
    "bert_config_L3.json refers to a 3-layer Bert.\n",
    "\n",
    "bert_config.json refers to a standard 12-layer Bert."
   ],
   "metadata": {
    "id": "4emuX8UK8Mup"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bert_config_T3 = BertConfig.from_json_file('/content/drive/MyDrive/TextBrewer-master/examples/student_config/bert_base_cased_config/bert_config_L3.json')#相对路径\r\n",
    "bert_config_T3.output_hidden_states = True\r\n",
    "\r\n",
    "student_model = BertForSequenceClassification(bert_config_T3) #, num_labels = 2\r\n",
    "student_model.to(device=device)\r\n",
    "\r\n",
    "\r\n",
    "bert_config = BertConfig.from_json_file('/content/drive/MyDrive/TextBrewer-master/examples/student_config/bert_base_cased_config/bert_config.json')\r\n",
    "bert_config.output_hidden_states = True\r\n",
    "teacher_model = BertForSequenceClassification(bert_config) #, num_labels = 2\r\n",
    "teacher_model.load_state_dict(torch.load('/content/drive/MyDrive/sst2_teacher_model.pt'))\r\n",
    "teacher_model.to(device=device)"
   ],
   "outputs": [],
   "metadata": {
    "id": "CKLaqSPCiX1a"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cell below is to distill the teacher model to student model you prepared.\n",
    "\n",
    "After the code execution is complete, the distilled model will be in 'saved_model' in colab file list"
   ],
   "metadata": {
    "id": "W6SuVnpa8RAm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_epochs = 20\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-4)\n",
    "\n",
    "scheduler_class = get_linear_schedule_with_warmup\n",
    "# arguments dict except 'optimizer'\n",
    "scheduler_args = {'num_warmup_steps':int(0.1*num_training_steps), 'num_training_steps':num_training_steps}\n",
    "\n",
    "\n",
    "def simple_adaptor(batch, model_outputs):\n",
    "    return {'logits': model_outputs.logits, 'hidden': model_outputs.hidden_states}\n",
    "\n",
    "distill_config = DistillationConfig(\n",
    "    intermediate_matches=[    \n",
    "     {'layer_T':0, 'layer_S':0, 'feature':'hidden', 'loss': 'hidden_mse','weight' : 1},\n",
    "     {'layer_T':8, 'layer_S':2, 'feature':'hidden', 'loss': 'hidden_mse','weight' : 1}])\n",
    "train_config = TrainingConfig()\n",
    "\n",
    "distiller = GeneralDistiller(\n",
    "    train_config=train_config, distill_config=distill_config,\n",
    "    model_T=teacher_model, model_S=student_model, \n",
    "    adaptor_T=simple_adaptor, adaptor_S=simple_adaptor)\n",
    "\n",
    "\n",
    "with distiller:\n",
    "    distiller.train(optimizer, train_dataloader, num_epochs, scheduler_class=scheduler_class, scheduler_args = scheduler_args, callback=None)"
   ],
   "outputs": [],
   "metadata": {
    "id": "CIxaegSUikGX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_model = BertForSequenceClassification(bert_config_T3)\n",
    "test_model.load_state_dict(torch.load('/content/drive/MyDrive/gs4210.pkl'))#gs4210 is the distilled model weights file"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 65
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8acpGydEgLf",
    "outputId": "79a0c44f-7f03-4d6d-b09b-84a858fa1360"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=8)"
   ],
   "outputs": [],
   "metadata": {
    "id": "_5QYCFnAMkpE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metric= load_metric(\"accuracy\")\n",
    "test_model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = test_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ],
   "outputs": [],
   "metadata": {
    "id": "u0Pb4CeJdLCk"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPSRVIK8638b2CgsGZ/nsAR",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1LgVQBkBlDbyTgriuZ88TDXPA6MSUKN3W",
   "name": "sst2_bert_fin.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('nlp': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "a447bb19d79f3db2674aacc85e780e48e431a75fa5f05809ae9b8d2d81dec273"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}